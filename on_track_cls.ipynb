{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46594ffd-85ed-482d-bed2-4c4cbd93317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\auyin11\\anaconda3\\envs\\rl_game_pip\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\auyin11\\anaconda3\\envs\\rl_game_pip\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import wandb\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "\n",
    "from matplotlib import image\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from cfgs.on_track_cls import OnTrackClsCfg\n",
    "from dp_util.early_stopping import EarlyStopper\n",
    "from models.on_track_cls import OnTrackClsNet\n",
    "from dp_spec.img import preprocess_img\n",
    "from dp_util.dataset import CustomDataset\n",
    "from dp_util.training import valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d541fa20-6915-4f79-8fa3-d2f6678017b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mauyin11\u001b[0m (\u001b[33mkaggle_winner\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\auyin11\\PycharmProjects\\rl_game\\wandb\\run-20230717_234843-ubr6glng</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaggle_winner/on_track_classifier/runs/ubr6glng' target=\"_blank\">v_2_0_5</a></strong> to <a href='https://wandb.ai/kaggle_winner/on_track_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaggle_winner/on_track_classifier' target=\"_blank\">https://wandb.ai/kaggle_winner/on_track_classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaggle_winner/on_track_classifier/runs/ubr6glng' target=\"_blank\">https://wandb.ai/kaggle_winner/on_track_classifier/runs/ubr6glng</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "dir_auto_img = r'D:\\file\\data\\racing\\auto_img'\n",
    "\n",
    "cfg = OnTrackClsCfg(test_mode=False, dir_data='./')\n",
    "\n",
    "wandb.init(project=cfg.project, entity=cfg.entity,\n",
    "           group=f'{cfg.user}_{cfg.model}', job_type=\"train\",\n",
    "           name= cfg.version)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "889ded94-7fee-4982-9ccc-229928ab1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_path = []\n",
    "list_lbl = []\n",
    "\n",
    "for file in os.listdir(os.path.join(dir_auto_img, 'on')):\n",
    "    list_path.append(os.path.join(dir_auto_img, 'on', file))\n",
    "    list_lbl.append(['on'])\n",
    "\n",
    "for file in os.listdir(os.path.join(dir_auto_img, 'off')):\n",
    "    list_path.append(os.path.join(dir_auto_img, 'off', file))\n",
    "    list_lbl.append(['off'])\n",
    "    \n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "list_lbl_array = encoder.fit_transform(list_lbl).toarray()\n",
    "\n",
    "\n",
    "path_train, path_test, y_train, y_test = train_test_split(list_path, list_lbl_array, test_size=0.2, stratify=list_lbl, shuffle=True) \n",
    "path_train, path_valid, y_train, y_valid = train_test_split(path_train, y_train, test_size=0.2, stratify=y_train, shuffle=True) \n",
    "\n",
    "ts_y_train = torch.tensor(y_train, device=device)\n",
    "ts_y_valid = torch.tensor(y_valid, device=device)\n",
    "ts_y_test = torch.tensor(y_test, device=device)\n",
    "\n",
    "list_x_y = []\n",
    "for x, y in  zip([torch.tensor(preprocess_img(image.imread(x)),\n",
    "                               dtype=torch.float32) for x in path_train], ts_y_train):\n",
    "    list_x_y.append((x, y))\n",
    "train_loader = torch.utils.data.DataLoader(dataset=CustomDataset(list_x_y),\n",
    "                                           batch_size=cfg.BATCH_SIZE,\n",
    "                                           shuffle=False)\n",
    "list_x_y = []\n",
    "for x, y in  zip([torch.tensor(preprocess_img(image.imread(x)),\n",
    "                               dtype=torch.float32) for x in path_valid], ts_y_valid):\n",
    "    list_x_y.append((x, y))\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=CustomDataset(list_x_y),\n",
    "                                           batch_size=cfg.BATCH_SIZE,\n",
    "                                           shuffle=False)\n",
    "\n",
    "list_x_y = []\n",
    "for x, y in  zip([torch.tensor(preprocess_img(image.imread(x)),\n",
    "                               dtype=torch.float32) for x in path_test], ts_y_test):\n",
    "    list_x_y.append((x, y))\n",
    "test_loader = torch.utils.data.DataLoader(dataset=CustomDataset(list_x_y),\n",
    "                                           batch_size=cfg.BATCH_SIZE,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e6fdfc-7f15-488c-b138-95139a5e53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = OnTrackClsNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=cfg.LR)\n",
    "\n",
    "es = EarlyStopper(cfg.md_name, smaller_is_better=True, patience=30, sf=cfg.SF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aff04825-3cf8-4289-9a22-de2641bac9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the value is started at 0.02260\n",
      "the value is improved from 0.02107 to 0.01760 (best_epoch: 5)\n",
      "the value is improved from 0.00561 to 0.00514 (best_epoch: 10)\n",
      "the value is improved from 0.00448 to 0.00444 (best_epoch: 15)\n",
      "the value is improved from 0.00402 to 0.00399 (best_epoch: 20)\n",
      "the value is improved from 0.00342 to 0.00336 (best_epoch: 25)\n",
      "the value is improved from 0.00305 to 0.00292 (best_epoch: 30)\n",
      "the value is improved from 0.00264 to 0.00259 (best_epoch: 35)\n",
      "the value is improved from 0.00240 to 0.00237 (best_epoch: 40)\n",
      "the value is improved from 0.00217 to 0.00214 (best_epoch: 45)\n",
      "the value is improved from 0.00203 to 0.00191 (best_epoch: 50)\n",
      "\n",
      "Early stop at best_epoch = 51 and best_value = 0.00180\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        inputs = torch.unsqueeze(inputs, dim=1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if not es.early_stop:\n",
    "        \n",
    "        train_avg_loss = running_loss/total\n",
    "        valid_avg_loss, _ = valid(net, valid_loader, criterion,\n",
    "                                  device, verbose=False)\n",
    "        wandb.log({'train_avg_loss': train_avg_loss, 'valid_avg_loss': valid_avg_loss})\n",
    "        \n",
    "        es(valid_avg_loss, net)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd8beb02-7e90-4dee-9d30-9fbdac7a7a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 190 images: 98 % ang avg_loss: 0.00426\n"
     ]
    }
   ],
   "source": [
    "test_avg_loss, _ = valid(net, test_loader, criterion, device)\n",
    "wandb.log({'test_avg_loss': test_avg_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19754bce-5dd8-4943-868a-24dcc2633dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ont_hot_encoder.pkl','wb') as output:\n",
    "      pickle.dump(encoder, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fbc4c46-012d-4ab4-9031-957c0b4268da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on_track_classifier_v_2_0_5.pth'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.md_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a7fa2-b75a-41e2-94c6-23d3eb38ed7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_game_pip",
   "language": "python",
   "name": "rl_game_pip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
